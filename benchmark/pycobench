#!/usr/bin/env python3
#
# pycobench - a small benchmarking solution
#
# A small environment for running benchmarks
#
# If you need something more elaborate, check, e.g., benchexec
#   ( https://github.com/sosy-lab/benchexec )

import argparse
import os
import queue
import subprocess
import sys
import threading
import yaml

# thread-safe queue for distributing tasks
g_task_queue = queue.Queue()

# thread-safe queue for collecting results
g_result_queue = queue.Queue()

# a dictionary of commands to run
g_cmd_dict = { }

# timeout for subprocesses (in seconds)
g_timeout = 600

###########################################
def execute_benchmark(params):
    '''execute_benchmark(params) -> void

Executes one benchmark.
'''
    global g_cmd_dict
    name, in_params = params
    cmd = g_cmd_dict[name]['cmd']
    cmd = cmd.split()
    in_params = in_params.split()

    # substitute $1, $2, etc. with the real parameters
    for i in range(len(cmd)):
        if len(cmd[i]) == 2 and cmd[i][0] == '$':
            try:
                x = int(cmd[i][1])
            except:
                raise Exception('invalid placeholder \"' + cmd[i] + '\" in the command to run')

            if x > len(in_params):
                raise Exception('parameter ' + cmd[i] + ' not provided (only ' +
                    str(len(in_params)) + ' parameters passed)')
            else:
                cmd[i] = in_params[x-1]

    try:
        # subprocess.run(["sleep", "10"], timeout=g_timeout)
        proc = subprocess.run(cmd, timeout=g_timeout, capture_output=True)
        print(str(proc))
        return 42 # TODO
    except subprocess.TimeoutExpired:
        return 'Timeout'


###########################################
def worker():
    '''worker() -> void

Main function of a thread for processing tasks.
'''
    while True:
        item = g_task_queue.get()
        if item is None:     # None signals end of processing
            g_result_queue.put(None)   # signal termination of worker
            break
        res = execute_benchmark(item)
        g_result_queue.put((item, res))
        g_task_queue.task_done()


###########################################
def process_input_line(line):
    '''process_input_line(str) -> void

Processes one input line.
'''
    for k in g_cmd_dict:
        g_task_queue.put((k, line))


###########################################
def process_result(result):
    '''process_result(result) -> void

Processes one obtained result.
'''
    print(str(result))


###########################################
def process_conf_file(conf_file):
    '''process_conf_file(conf_file) -> void

Processes the configuration file.
'''
    global g_cmd_dict
    config = yaml.load(conf_file, Loader=yaml.FullLoader)
    g_cmd_dict = config


###########################################
def run_main(args):
    '''run_main(args) -> ()

Runs the main program according to the arguments obtained from the parser.
'''
    assert(len(args.conf) == 1)
    with open(args.conf[0], 'r') as conf_file:
        process_conf_file(conf_file)

    print('continuing...')

    # set
    num_worker_threads = args.jobs
    if num_worker_threads is None:
        num_worker_threads = 1

    threads = []
    for i in range(num_worker_threads):
        t = threading.Thread(target=worker)
        t.start()
        threads.append(t)

    # processing the input
    for line in sys.stdin:
        line = line.rstrip()
        process_input_line(line)

    # send the END OF TASKS message
    for t in threads:
        g_task_queue.put(None)

    # processing the results
    finished_workers = 0
    while finished_workers < len(threads):
        result = g_result_queue.get()
        if result is None:
            print("worker terminated")
            finished_workers += 1
            continue
        process_result(result)

    # a barrier
    for t in threads:
        t.join()


###########################################
if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='''pycobench - \
Executes benchmarks given using a configuration file on cases given on the \
standard input''')
    parser.add_argument('conf', metavar='file.conf', nargs=1,
        help='configuration file')
    parser.add_argument('-j', '--jobs', type=int, default = os.cpu_count(),
        help='The number of jobs (workers) to run concurrently (default: %(default)s)')

    args = parser.parse_args()
    run_main(args)

# WHAT I WANT:
#   * make a queue with requests
#   * clients will read from the queue
#   * when killed, possible to restart
